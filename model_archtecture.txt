
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from transformers import (
    BartForConditionalGeneration, BartTokenizer, BartConfig,
    T5ForConditionalGeneration, T5Tokenizer
)

class TextSummarizationModel:
    
    
    def __init__(self, model_type="bart", model_name=None):
        self.model_type = model_type
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        if model_type == "bart":
            self.model_name = model_name or "facebook/bart-large-cnn"
            print(f"Loading BART model: {self.model_name}")
            self.model = BartForConditionalGeneration.from_pretrained(self.model_name)
            self.tokenizer = BartTokenizer.from_pretrained(self.model_name)
            
        elif model_type == "t5":
            self.model_name = model_name or "t5-small"
            print(f"Loading T5 model: {self.model_name}")
            self.model = T5ForConditionalGeneration.from_pretrained(self.model_name)
            self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)
            
        elif model_type == "custom":
            print("Initializing custom transformer model...")
            self.model = CustomSummarizer()
            self.tokenizer = None
        
        print(f"Model loaded successfully! Trainable parameters: {self.count_parameters():,}")
    
    def count_parameters(self):
        
        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)
    
    def generate_summary(self, text, max_length=150, min_length=30, num_beams=4):
        
        
        inputs = self.tokenizer(
            text, 
            max_length=1024, 
            truncation=True, 
            padding=True, 
            return_tensors="pt"
        ).to(self.device)
        
        
        summary_ids = self.model.generate(
            inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            max_length=max_length,
            min_length=min_length,
            num_beams=num_beams,
            early_stopping=True,
            no_repeat_ngram_size=2,
            length_penalty=2.0,
            temperature=0.8
        )
        
        
        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        return summary

class CustomSummarizer(nn.Module):
    
    
    def __init__(self, vocab_size=50265, d_model=512, nhead=8, num_layers=6):
        super().__init__()
        self.d_model = d_model
        
        
        self.encoder_embedding = nn.Embedding(vocab_size, d_model)
        self.decoder_embedding = nn.Embedding(vocab_size, d_model)
        
        
        self.pos_encoder = PositionalEncoding(d_model)
        self.pos_decoder = PositionalEncoding(d_model)
        
        
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_layers,
            num_decoder_layers=num_layers,
            dim_feedforward=2048,
            dropout=0.1
        )
        
        
        self.output_layer = nn.Linear(d_model, vocab_size)
        
    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        
        src = self.pos_encoder(self.encoder_embedding(src) * math.sqrt(self.d_model))
        tgt = self.pos_decoder(self.decoder_embedding(tgt) * math.sqrt(self.d_model))
        
        
        output = self.transformer(src, tgt, src_mask, tgt_mask)
        output = self.output_layer(output)
        
        return output

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]