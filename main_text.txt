
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AdamW, get_linear_schedule_with_warmup
import evaluate
from data_loader import get_dataloaders
from model import TextSummarizationModel
from config import Config
import os

class SummarizationTrainer:
    def __init__(self, model, train_dataloader, val_dataloader, device):
        self.model = model
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        self.device = device
        
        
        self.optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer, 
            num_warmup_steps=100, 
            num_training_steps=len(train_dataloader) * Config.NUM_EPOCHS
        )
        
        
        self.criterion = nn.CrossEntropyLoss(ignore_index=model.tokenizer.pad_token_id)
        
        
        self.rouge = evaluate.load('rouge')
        
        
        os.makedirs(Config.MODEL_SAVE_PATH, exist_ok=True)
    
    def train_epoch(self, epoch):
        self.model.train()
        total_loss = 0
        
        for batch_idx, batch in enumerate(self.train_dataloader):
            
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            
            outputs = self.model.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            
            
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.model.parameters(), 1.0)
            self.optimizer.step()
            self.scheduler.step()
            
            total_loss += loss.item()
            
            if batch_idx % 50 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        return total_loss / len(self.train_dataloader)
    
    def evaluate(self, epoch):
        self.model.model.eval()
        all_predictions = []
        all_references = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(self.val_dataloader):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels']
                
                
                summary_ids = self.model.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=Config.MAX_TARGET_LENGTH,
                    num_beams=Config.NUM_BEAMS,
                    length_penalty=Config.LENGTH_PENALTY,
                    early_stopping=True
                )
                
                
                predictions = self.model.tokenizer.batch_decode(
                    summary_ids, skip_special_tokens=True
                )
                references = self.model.tokenizer.batch_decode(
                    labels, skip_special_tokens=True
                )
                
                all_predictions.extend(predictions)
                all_references.extend(references)
                
                
                if batch_idx == 0:
                    print("\n=== Sample Predictions ===")
                    for i in range(min(2, len(predictions))):
                        print(f"Reference: {references[i][:100]}...")
                        print(f"Prediction: {predictions[i]}")
                        print("---")
        
        
        rouge_scores = self.rouge.compute(
            predictions=all_predictions, 
            references=all_references,
            use_stemmer=True
        )
        
        print(f"\nEpoch {epoch} - ROUGE Scores:")
        for key, value in rouge_scores.items():
            print(f"  {key}: {value:.4f}")
        
        return rouge_scores
    
    def train(self):
        best_rouge = 0
        
        for epoch in range(Config.NUM_EPOCHS):
            print(f"\n{'='*50}")
            print(f"Epoch {epoch + 1}/{Config.NUM_EPOCHS}")
            print(f"{'='*50}")
            
            
            train_loss = self.train_epoch(epoch)
            print(f"Training Loss: {train_loss:.4f}")
            
            
            rouge_scores = self.evaluate(epoch)
            current_rouge = rouge_scores['rougeL']
            
            
            if current_rouge > best_rouge:
                best_rouge = current_rouge
                model_path = f"{Config.MODEL_SAVE_PATH}best_model.pth"
                torch.save(self.model.model.state_dict(), model_path)
                print(f"âœ… New best model saved! ROUGE-L: {best_rouge:.4f}")
            
            
            checkpoint_path = f"{Config.MODEL_SAVE_PATH}checkpoint_epoch_{epoch}.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'rouge_score': current_rouge,
                'loss': train_loss
            }, checkpoint_path)
        
        print(f"\n Training completed! Best ROUGE-L: {best_rouge:.4f}")

def main():
    
    print(" Starting Text Summarization Training")
    
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    
    print("Loading model...")
    model_wrapper = TextSummarizationModel(
        model_type=Config.MODEL_TYPE,
        model_name=Config.PRETRAINED_MODEL
    )
    model_wrapper.model.to(device)
    
    
    print("Loading dataset...")
    train_dataloader, val_dataloader = get_dataloaders(
        model_wrapper.tokenizer,
        batch_size=Config.BATCH_SIZE
    )
    
    print(f"Training batches: {len(train_dataloader)}")
    print(f"Validation batches: {len(val_dataloader)}")
    
    
    trainer = SummarizationTrainer(
        model=model_wrapper,
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        device=device
    )
    
    
    trainer.train()

if __name__ == "__main__":
    main()